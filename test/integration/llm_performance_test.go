package integration

import (
	"context"
	"os"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"memoro/internal/config"
	"memoro/internal/models"
	"memoro/internal/services/llm"
)

// TestLLMPerformance æµ‹è¯•LLMæ€§èƒ½
func TestLLMPerformance(t *testing.T) {
	// æ£€æŸ¥API Key
	apiKey := os.Getenv("MEMORO_LLM_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping LLM API test: MEMORO_LLM_API_KEY not set")
	}

	// åŠ è½½é…ç½®
	_, err := config.Load("../../config/app.yaml")
	require.NoError(t, err)

	client, err := llm.NewClient()
	require.NoError(t, err)

	t.Run("åŸºç¡€å“åº”æ—¶é—´æµ‹è¯•", func(t *testing.T) {
		ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
		defer cancel()

		// æµ‹è¯•ç®€å•è¯·æ±‚çš„å“åº”æ—¶é—´
		startTime := time.Now()
		
		messages := []llm.ChatMessage{
			{Role: "user", Content: "ä½ å¥½"},
		}

		response, err := client.ChatCompletion(ctx, messages)
		duration := time.Since(startTime)

		require.NoError(t, err, "åŸºç¡€è¯·æ±‚åº”è¯¥æˆåŠŸ")
		require.NotNil(t, response, "å“åº”ä¸åº”ä¸ºç©º")

		// åŸºç¡€æ€§èƒ½æ–­è¨€ï¼ˆæ ¹æ®third-part-ai.mdçš„APIæ€§èƒ½é¢„æœŸï¼‰
		assert.Less(t, duration, 30*time.Second, "ç®€å•è¯·æ±‚åº”åœ¨30ç§’å†…å®Œæˆ")
		assert.Greater(t, response.Usage.TotalTokens, 0, "åº”è¯¥æœ‰tokenä½¿ç”¨ç»Ÿè®¡")

		t.Logf("âœ… åŸºç¡€å“åº”æ—¶é—´: %v", duration)
		t.Logf("ğŸ“Š Tokenä½¿ç”¨: %d total", response.Usage.TotalTokens)
	})

	t.Run("æ‘˜è¦ç”Ÿæˆæ€§èƒ½æµ‹è¯•", func(t *testing.T) {
		summarizer, err := llm.NewSummarizer(client)
		require.NoError(t, err)

		ctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)
		defer cancel()

		testContent := `äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯ç ”ç©¶ã€å¼€å‘ç”¨äºæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººçš„æ™ºèƒ½çš„ç†è®ºã€æ–¹æ³•ã€æŠ€æœ¯åŠåº”ç”¨ç³»ç»Ÿçš„ä¸€é—¨æ–°çš„æŠ€æœ¯ç§‘å­¦ã€‚äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ï¼Œå¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚

è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„çªç ´æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚ä»è¯­éŸ³åŠ©æ‰‹åˆ°è‡ªåŠ¨é©¾é©¶ï¼Œä»æ¨èç³»ç»Ÿåˆ°æ™ºèƒ½å®¢æœï¼ŒAIæ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°æ›´æ˜¯è®©AIåœ¨ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–¹é¢è¾¾åˆ°äº†æ–°çš„é«˜åº¦ã€‚`

		startTime := time.Now()

		request := llm.SummaryRequest{
			Content:     testContent,
			ContentType: models.ContentTypeText,
		}

		result, err := summarizer.GenerateSummary(ctx, request)
		duration := time.Since(startTime)

		require.NoError(t, err, "æ‘˜è¦ç”Ÿæˆåº”è¯¥æˆåŠŸ")
		require.NotNil(t, result, "æ‘˜è¦ç»“æœä¸åº”ä¸ºç©º")

		// æ€§èƒ½æ–­è¨€
		assert.Less(t, duration, 90*time.Second, "æ‘˜è¦ç”Ÿæˆåº”åœ¨90ç§’å†…å®Œæˆ")
		assert.NotEmpty(t, result.OneLine, "ä¸€å¥è¯æ‘˜è¦ä¸åº”ä¸ºç©º")
		assert.NotEmpty(t, result.Paragraph, "æ®µè½æ‘˜è¦ä¸åº”ä¸ºç©º")
		assert.NotEmpty(t, result.Detailed, "è¯¦ç»†æ‘˜è¦ä¸åº”ä¸ºç©º")

		t.Logf("âœ… æ‘˜è¦ç”Ÿæˆæ€§èƒ½: %v", duration)
		t.Logf("ğŸ“ ç”Ÿæˆç»“æœé•¿åº¦: ä¸€å¥è¯=%d, æ®µè½=%d, è¯¦ç»†=%d", 
			len(result.OneLine), len(result.Paragraph), len(result.Detailed))
	})

	t.Run("æ ‡ç­¾ç”Ÿæˆæ€§èƒ½æµ‹è¯•", func(t *testing.T) {
		tagger, err := llm.NewTagger(client)
		require.NoError(t, err)

		ctx, cancel := context.WithTimeout(context.Background(), 90*time.Second)
		defer cancel()

		testContent := `æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹å°±èƒ½åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æ·±åº¦å­¦ä¹ ä½œä¸ºæœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚æ•°æ®ã€‚è¿™äº›æŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚`

		startTime := time.Now()

		request := llm.TagRequest{
			Content:     testContent,
			ContentType: models.ContentTypeText,
			MaxTags:     10,
		}

		result, err := tagger.GenerateTags(ctx, request)
		duration := time.Since(startTime)

		require.NoError(t, err, "æ ‡ç­¾ç”Ÿæˆåº”è¯¥æˆåŠŸ")
		require.NotNil(t, result, "æ ‡ç­¾ç»“æœä¸åº”ä¸ºç©º")

		// æ€§èƒ½å’Œè´¨é‡æ–­è¨€
		assert.Less(t, duration, 60*time.Second, "æ ‡ç­¾ç”Ÿæˆåº”åœ¨60ç§’å†…å®Œæˆ")
		assert.NotEmpty(t, result.Tags, "æ ‡ç­¾åˆ—è¡¨ä¸åº”ä¸ºç©º")
		assert.LessOrEqual(t, len(result.Tags), 10, "æ ‡ç­¾æ•°é‡ä¸åº”è¶…è¿‡é™åˆ¶")

		t.Logf("âœ… æ ‡ç­¾ç”Ÿæˆæ€§èƒ½: %v", duration)
		t.Logf("ğŸ·ï¸ ç”Ÿæˆç»“æœ: æ ‡ç­¾=%dä¸ª, åˆ†ç±»=%dä¸ª, å…³é”®è¯=%dä¸ª", 
			len(result.Tags), len(result.Categories), len(result.Keywords))
	})
}

// TestLLMConcurrentPerformance æµ‹è¯•å¹¶å‘æ€§èƒ½
func TestLLMConcurrentPerformance(t *testing.T) {
	// æ£€æŸ¥API Key
	apiKey := os.Getenv("MEMORO_LLM_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping LLM API test: MEMORO_LLM_API_KEY not set")
	}

	// åŠ è½½é…ç½®
	_, err := config.Load("../../config/app.yaml")
	require.NoError(t, err)

	t.Run("å¹¶å‘æ‘˜è¦ç”Ÿæˆæµ‹è¯•", func(t *testing.T) {
		concurrentRequests := 3 // é¿å…è¶…å‡ºAPIé€Ÿç‡é™åˆ¶
		results := make(chan TestResult, concurrentRequests)

		testContents := []string{
			"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆæˆ·å¤–æ´»åŠ¨ã€‚",
			"äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚",
			"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ï¼Œå…·æœ‰å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚",
		}

		startTime := time.Now()

		// å¯åŠ¨å¹¶å‘è¯·æ±‚
		for i := 0; i < concurrentRequests; i++ {
			go func(index int) {
				client, err := llm.NewClient()
				if err != nil {
					results <- TestResult{Error: err, Duration: 0}
					return
				}

				summarizer, err := llm.NewSummarizer(client)
				if err != nil {
					results <- TestResult{Error: err, Duration: 0}
					return
				}

				ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
				defer cancel()

				requestStart := time.Now()

				request := llm.SummaryRequest{
					Content:     testContents[index%len(testContents)],
					ContentType: models.ContentTypeText,
				}

				_, err = summarizer.GenerateSummary(ctx, request)
				duration := time.Since(requestStart)

				results <- TestResult{Error: err, Duration: duration}
			}(i)
		}

		// æ”¶é›†ç»“æœ
		successCount := 0
		var totalDuration time.Duration
		for i := 0; i < concurrentRequests; i++ {
			result := <-results
			if result.Error == nil {
				successCount++
				totalDuration += result.Duration
			} else {
				t.Logf("å¹¶å‘è¯·æ±‚ %d å¤±è´¥: %v", i+1, result.Error)
			}
		}

		overallDuration := time.Since(startTime)

		// å¹¶å‘æ€§èƒ½æ–­è¨€
		assert.Greater(t, successCount, 0, "è‡³å°‘åº”æœ‰ä¸€ä¸ªè¯·æ±‚æˆåŠŸ")
		
		if successCount > 0 {
			avgDuration := totalDuration / time.Duration(successCount)
			t.Logf("âœ… å¹¶å‘æ‘˜è¦æµ‹è¯•å®Œæˆ")
			t.Logf("ğŸ“Š æˆåŠŸç‡: %d/%d", successCount, concurrentRequests)
			t.Logf("â±ï¸ æ€»è€—æ—¶: %v", overallDuration)
			t.Logf("â±ï¸ å¹³å‡å•è¯·æ±‚è€—æ—¶: %v", avgDuration)
		}
	})

	t.Run("å¹¶å‘æ ‡ç­¾ç”Ÿæˆæµ‹è¯•", func(t *testing.T) {
		concurrentRequests := 2 // ä¿æŒè¾ƒä½å¹¶å‘ä»¥é¿å…APIé™åˆ¶
		results := make(chan TestResult, concurrentRequests)

		testContents := []string{
			"æœºå™¨å­¦ä¹ ç®—æ³•åœ¨æ•°æ®ç§‘å­¦ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚",
			"äº‘è®¡ç®—ä¸ºä¼ä¸šæä¾›äº†çµæ´»çš„ITåŸºç¡€è®¾æ–½ã€‚",
		}

		for i := 0; i < concurrentRequests; i++ {
			go func(index int) {
				client, err := llm.NewClient()
				if err != nil {
					results <- TestResult{Error: err, Duration: 0}
					return
				}

				tagger, err := llm.NewTagger(client)
				if err != nil {
					results <- TestResult{Error: err, Duration: 0}
					return
				}

				ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
				defer cancel()

				requestStart := time.Now()

				request := llm.TagRequest{
					Content:     testContents[index%len(testContents)],
					ContentType: models.ContentTypeText,
					MaxTags:     5,
				}

				_, err = tagger.GenerateTags(ctx, request)
				duration := time.Since(requestStart)

				results <- TestResult{Error: err, Duration: duration}
			}(i)
		}

		// æ”¶é›†ç»“æœ
		successCount := 0
		for i := 0; i < concurrentRequests; i++ {
			result := <-results
			if result.Error == nil {
				successCount++
			}
		}

		assert.Greater(t, successCount, 0, "è‡³å°‘åº”æœ‰ä¸€ä¸ªæ ‡ç­¾ç”Ÿæˆè¯·æ±‚æˆåŠŸ")
		t.Logf("âœ… å¹¶å‘æ ‡ç­¾æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: %d/%d", successCount, concurrentRequests)
	})
}

// TestLLMTokenUsageAccuracy æµ‹è¯•Tokenä½¿ç”¨ç»Ÿè®¡å‡†ç¡®æ€§
func TestLLMTokenUsageAccuracy(t *testing.T) {
	// æ£€æŸ¥API Key
	apiKey := os.Getenv("MEMORO_LLM_API_KEY")
	if apiKey == "" {
		t.Skip("Skipping LLM API test: MEMORO_LLM_API_KEY not set")
	}

	// åŠ è½½é…ç½®
	_, err := config.Load("../../config/app.yaml")
	require.NoError(t, err)

	client, err := llm.NewClient()
	require.NoError(t, err)

	t.Run("Tokenç»Ÿè®¡å‡†ç¡®æ€§æµ‹è¯•", func(t *testing.T) {
		ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer cancel()

		testCases := []struct {
			name     string
			content  string
			expected TokenExpectation
		}{
			{
				name:    "çŸ­æ¶ˆæ¯",
				content: "ä½ å¥½",
				expected: TokenExpectation{MinPrompt: 1, MaxPrompt: 10, MinCompletion: 1, MaxCompletion: 20},
			},
			{
				name:    "ä¸­ç­‰é•¿åº¦æ¶ˆæ¯",
				content: "è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ",
				expected: TokenExpectation{MinPrompt: 5, MaxPrompt: 30, MinCompletion: 10, MaxCompletion: 100},
			},
		}

		for _, tc := range testCases {
			t.Run(tc.name, func(t *testing.T) {
				messages := []llm.ChatMessage{
					{Role: "user", Content: tc.content},
				}

				response, err := client.ChatCompletion(ctx, messages)
				require.NoError(t, err, "è¯·æ±‚åº”è¯¥æˆåŠŸ")

				usage := response.Usage

				// éªŒè¯Tokenç»Ÿè®¡çš„åˆç†æ€§
				assert.GreaterOrEqual(t, usage.PromptTokens, tc.expected.MinPrompt, "Prompt tokensåº”è¯¥åœ¨åˆç†èŒƒå›´å†…")
				assert.LessOrEqual(t, usage.PromptTokens, tc.expected.MaxPrompt, "Prompt tokensåº”è¯¥åœ¨åˆç†èŒƒå›´å†…")
				assert.GreaterOrEqual(t, usage.CompletionTokens, tc.expected.MinCompletion, "Completion tokensåº”è¯¥åœ¨åˆç†èŒƒå›´å†…")
				assert.LessOrEqual(t, usage.CompletionTokens, tc.expected.MaxCompletion, "Completion tokensåº”è¯¥åœ¨åˆç†èŒƒå›´å†…")
				assert.Equal(t, usage.TotalTokens, usage.PromptTokens+usage.CompletionTokens, "Total tokensåº”è¯¥ç­‰äºprompt + completion")

				t.Logf("ğŸ“Š %s Tokenä½¿ç”¨: %d prompt + %d completion = %d total", 
					tc.name, usage.PromptTokens, usage.CompletionTokens, usage.TotalTokens)
			})
		}
	})
}

// TestResult æµ‹è¯•ç»“æœç»“æ„
type TestResult struct {
	Error    error
	Duration time.Duration
}

// TokenExpectation Tokenä½¿ç”¨é¢„æœŸ
type TokenExpectation struct {
	MinPrompt     int
	MaxPrompt     int
	MinCompletion int
	MaxCompletion int
}